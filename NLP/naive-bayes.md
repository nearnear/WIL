
ì´ ê¸€ì€ deeplearning.aiì˜ NLP Specializationë¥¼ ì°¸ê³ í•˜ì—¬ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì„ í…ìŠ¤íŠ¸ ì •ì„œ ë¶„ì„ì— ì´ˆì ì„ ë§ì¶° ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.

> [Githubì—ì„œ Naive Bayes ì½”ë“œ ë³´ê¸°](https://github.com/snowith/nlp_model_practices/blob/main/naive_bayes/naive_bayes_sentiment.ipynb)


## 0. ëª¨ë¸ ê°œëµ
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì€ ë¶„ë¥˜ ê³¼ì œë¥¼ ìœ„í•œ í™•ë¥  ëª¨ë¸ì´ë‹¤. í›ˆë ¨ ë°ì´í„°ì— ë“±ì¥í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì„¸ì–´ì„œ ê° ë°ì´í„°ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ë¯€ë¡œ ë¶„ë¥˜ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ëŠ”ë° ì í•©í•˜ë‹¤.

### ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì€
- í›ˆë ¨ê³¼ ì˜ˆì¸¡ì„ ë¹ ë¥´ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ baseline ëª¨ë¸ë¡œ ì í•©í•˜ë‹¤.
- ë¬¸ì¥ì— ìˆëŠ” ê° ë‹¨ì–´ë“¤ì´ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì˜ ê´€ê³„ë¥¼ ì¸¡ì •í•˜ê±°ë‚˜ ë¬¸ì¥ ë‚´ì˜ ë¹ˆì¹¸ì„ ì±„ìš°ëŠ” ë“±ì˜ ê³¼ì œì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.
- í›ˆë ¨ ë°ì´í„° ë‚´ ë‹¨ì–´ë“¤ì´ ë“±ì¥í•˜ëŠ” ë¹ˆë„ì— ê¸°ë°˜í•˜ê¸° ë•Œë¬¸ì—, í›ˆë ¨ ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë‹¨ì–´ì— ëŒ€í•œ ì˜ˆì¸¡ì´ë‚˜ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ íŒë‹¨í•˜ëŠ” ê³¼ì œì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.
- ê°ì • ë¶„ì„, ì €ì ë¶„ë¥˜, ìŠ¤íŒ¸ í•„í„°ë§, ë¬¸ì„œ ìš”ì•½, ë™ìŒì´ì˜ì–´ êµ¬ë¶„ ë“±ì˜ ê³¼ì œì— í™œìš©í•  ìˆ˜ ìˆë‹¤.

ì´ ê¸€ì—ì„œëŠ” ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ë¡œ ì´ì§„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ìƒí™©ì„ ê°€ì •í•˜ê² ë‹¤. íŠ¹íˆ ì–´ë–¤ ë¬¸ì¥ì„ ì…ë ¥ ë°›ì•„ì„œ ë¬¸ì¥ì´ ê¸ì •ì ì¸ ì •ì„œë¥¼ ë‚´í¬í•˜ê³  ìˆìœ¼ë©´ `1`ì„, ë¶€ì •ì ì¸ ì •ì„œë¥¼ ë‚´í¬í•˜ê³  ìˆìœ¼ë©´ `0`ì„ ë°˜í™˜í•˜ëŠ” ê°ì • ë¶„ì„(sentiment analysis) ê³¼ì œë¥¼ ìˆ˜í–‰í•œë‹¤.

```python
input_s = 'This is my best day ever.'
model(input_s) # 1, ê¸ì •

input_s = 'the class was in a terrible mood...'
model(input_s) # 0, ë¶€ì •
```
_ìœ„ì™€ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ `model`ì„ ì–»ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤._



## 1. ì¡°ê±´ë¶€ í™•ë¥ ê³¼ ë² ì´ì¦ˆ ë£°
ëª¨ë¸ì„ ì‚´í´ë³´ê¸°ì— ì•ì„œ ì¡°ê±´ë¶€ í™•ë¥ ê³¼ ë² ì´ì¦ˆ ë£°ì— ëŒ€í•´ ì•Œì•„ë³´ì.

### ğŸ² ì¡°ê±´ë¶€ í™•ë¥ (Conditional Probability) ì´ë€?
ëª¨ìˆ˜ì—ì„œ ì¡°ê±´ Aê°€ ë§Œì¡±ë  í™•ë¥ ì„ $P(A)$, ì¡°ê±´ Bê°€ ë§Œì¡±ë  í™•ë¥ ì„ $P(B)$ë¼ê³  í•˜ì. ì´ë•Œ Bì— ëŒ€í•œ Aì˜ ì¡°ê±´ë¶€ í™•ë¥  $P(A|B)$ ëŠ” ì¡°ê±´ Bë¥¼ ë§Œì¡±í•˜ëŠ” í‘œë³¸ì—ì„œ ì¡°ê±´ Aë¥¼ ë§Œì¡±í•˜ëŠ” í‘œë³¸ì„ ì„ íƒí•  í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. ì¦‰ $P(A|B)$ëŠ” ì¡°ê±´ Aì™€ Bë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” í‘œë³¸ì„ ì„ íƒí•  í™•ë¥ ì¸ $P(A \cap B)$ì— ëª¨ìˆ˜ì—ì„œ ì¡°ê±´ Bë¥¼ ë§Œì¡±í•˜ëŠ” í‘œë³¸ì„ ì„ íƒí•  í™•ë¥  $P(B)$ë¥¼ ë‚˜ëˆˆ ê°’ìœ¼ë¡œ ì •ì˜ëœë‹¤.

$$
P(A|B) =\frac{P(A \cap B)}{P(B)}
$$

ì¡°ê±´ë¶€ í™•ë¥ ì€ ë½‘ì„ ìƒ˜í”Œì˜ ë²”ìœ„ë¥¼ í‘œë³¸ ëŒ€ì‹  ì¡°ê±´ìœ¼ë¡œ ì œí•œí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. 

### ğŸ² ë² ì´ì¦ˆ ì •ë¦¬ë€
ìœ„ì˜ ì •ì˜ë¡œ ë¶€í„° ë‘ê°œì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ í‘œí˜„ í•  ìˆ˜ ìˆë‹¤.

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \\
P(B|A) = \frac{P(A \cap B)}{P(A)} 
$$

ì˜ˆë¥¼ ë“¤ì–´ ì¡°ê±´ `A`ê°€ `20ëŒ€`ì´ê³  ì¡°ê±´ `B`ê°€ `ì‹¬ì¥ë³‘`ì´ë¼ê³  í•˜ì. ëª‡ ê°œì˜ ë³‘ì›ì—ì„œ í‘œë³¸ ì§‘ë‹¨ì„ ëª¨ì•„ì„œ **ì‹¬ì¥ë³‘ì— ê±¸ë¦° ì‚¬ëŒì´ 20ëŒ€ì¼ í™•ë¥ **ì„ ì¡°ì‚¬í•˜ê³ ì í•œë‹¤. ìš°ë¦¬ëŠ” í‘œë³¸ ì§‘ë‹¨ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë¶€í„° **ì‹¬ì¥ë³‘ì´ ê±¸ë¦° ì‚¬ëŒì˜ ë¹„ìœ¨**ê³¼ **20ëŒ€ì˜ ë¹„ìœ¨**ì„ ì•Œê³ ìˆìœ¼ë©°, ë‚˜ì•„ê°€ **20ëŒ€ ì¤‘ì—ì„œ ì‹¬ì¥ë³‘ì— ê±¸ë¦° ì‚¬ëŒì˜ ë¹„ìœ¨**ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ëŠ” ì„¸ê°€ì§€ ì •ë³´ë¡œë¶€í„° ì‹¬ì¥ë³‘ì— ê±¸ë¦° ì‚¬ëŒì´ 20ëŒ€ì¼ í™•ë¥ ì„ ë„ì¶œí•œë‹¤. ëŒ€ìˆ˜ ì—°ì‚°ì„ í†µí•´ $P(A|B)$ë¥¼ $P(B|A)$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$
P(A|B) = \frac{P(A)}{P(B)} \times P(B|A) ...... (*)
$$

ë‘ ì¡°ê±´ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì‹ (*)ì„ **ë² ì´ì¦ˆ ì •ë¦¬**ë¼ê³  í•œë‹¤. ì´ë•Œ $P(A)$ë¥¼ **ì‚¬ì „í™•ë¥ (prior)**, $P(A|B)$ë¥¼ **ì‚¬í›„í™•ë¥ (posterior)**, ê·¸ë¦¬ê³  $P(B|A)$ë¥¼ **ìš°ë„(likelihood)**ë¼ê³  ë¶€ë¥¸ë‹¤.


## 3. ì¡°ê±´ë¶€ ë¹ˆë„ ì„¸ê¸°
í…ìŠ¤íŠ¸ê°€ ë‚´í¬í•˜ëŠ” ê°ì •ì„ ì´ì§„ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ë¶„ë¥˜ í´ë˜ìŠ¤ë¥¼ $class \in \{positive, negative\}$ë¡œ ì •ì˜í•˜ì. $m$ê°œì˜ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” corpusì— ì†í•˜ëŠ” ë‹¨ì–´ $w_i \in corpus$ì— ëŒ€í•´ ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ê°’ì€ $P(class|w_i)$, ì¦‰ ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¨ì–´ê°€ íŠ¹ì • classì— ì†í•  í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ë– ì˜¬ë ¤ ë³´ë©´:

$$
P(class|w_i) = \frac{P(class) \cdot P(w_i|class)}{P(w_i)}
$$

ì´ë©° $P(w_i)$ëŠ” $w_i$ì— ëŒ€í•œ ìƒìˆ˜ê°’ì´ë¯€ë¡œ í™•ë¥ ì„ ê³„ì‚°í•  ë•Œ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤. ìš°ë¦¬ëŠ” ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ê° ë‹¨ì–´ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ê³„ì‚°í•´ì„œ ë‚˜ì´ë¸Œ ê°€ì •ì— ë”°ë¼ í•œ ë¬¸ì¥ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë°˜í™˜í•˜ê³ ì í•œë‹¤. ë”°ë¼ì„œ **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ** ëª¨ë¸ì˜ ì•„ì´ë””ì–´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ë¬¸ì¥ $sentence$ì— ì†í•œ ëª¨ë“  ë‹¨ì–´ $w_i \in sentence$ ($i=1, .., n$)ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤: 

$$
\hat{y} = argmax_{class} P(class) \prod_{i=1}^{n}P(w_i|class)
$$

ìœ„ ì‹ì€ ìµœëŒ€ ìš°ë„ ì¶”ì •(Maximum Likelihood Estimation, MLE)ì˜ ì•„ì´ë””ì–´ì´ê¸°ë„ í•˜ë‹¤. ìš°ì„ ì€ corpusì— ëŒ€í•œ ì¡°ê±´ë¶€ ë¹ˆë„ì¸ $P(w_i|class)$ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.

ë‹¤ì‹œ ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•´, í´ë˜ìŠ¤ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
P(w | class) &= \frac{P(w \cap class)}{P(class)} \\
&= \frac{freq(w, class)}{N_{class}}
\end{aligned}
$$

ìœ„ ì‹ì—ì„œ $freq(w, class)$ëŠ” $class$ì—ì„œ $w$ê°€ ë‚˜íƒ€ë‚˜ëŠ” íšŸìˆ˜ë¡œ, $P(w \cap class)$ì™€ ê°™ë‹¤. $N_{class}$ëŠ” í´ë˜ìŠ¤ì— í¬í•¨ë˜ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ë¹ˆë„ì´ë‹¤.

### ğŸ² Laplacian Smoothing
Laplacian Smoothingì€ ì¡°ê±´ë¶€ í™•ë¥ ì´ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ìœ„ì—ì„œ ë³¸ ìµœëŒ€ ìš°ë„ ì¶”ì •ì— ë”°ë¥´ë©´ ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ likelihoodë¥¼ ê³±í•˜ê²Œ ë˜ëŠ”ë°, ë§Œì•½ corpusì— ì—†ëŠ” ë‹¨ì–´ê°€ ë“¤ì–´ì˜¤ë©´ ë‹¤ë¥¸ íŠ¹ì„±ë“¤ì— ê´€ê³„ì—†ì´ ì˜ˆì¸¡ê°’ì´ 0ì´ ë  ê²ƒì´ë‹¤. ë¶„ìê°’ì— biasë¥¼ 1 ë”í•˜ë©´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. 

$$
P(w|class) = \frac{freq(w, class) + 1}{N_{class} + V_{class}}
$$

$V_{class}$ëŠ” í´ë˜ìŠ¤ì— ë“±ì¥í•˜ëŠ” **ìœ ì¼í•œ** ë‹¨ì–´ì˜ ê°œìˆ˜ì´ë‹¤. ë¶„ëª¨ì—ëŠ” $V_{class}$ë¥¼ ë”í•¨ìœ¼ë¡œì„œ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ likelihoodê°€ 1ì´ ë„˜ì§€ ì•Šë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆë‹¤:

$$
\sum_{w}P(w|class) = \frac{\sum_w freq(w,class) + V_{class}}{N_{class} + V_{class}}
$$



## 3. Likelihood ê³„ì‚°í•˜ê¸°
ì•ì—ì„œ í‘œí˜„í•œ ìµœëŒ€ ìš°ë„ ì¶”ì • ë°©ì‹ì„ ì¡°ê¸ˆ ë³€í˜•í•´, ì´ ê¸€ì—ì„œëŠ” **Likelihood-ratio** ë°©ë²•ì„ í†µí•´ ë¶„ë¥˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³ ì í•œë‹¤. 

ìš°ì„  ratioë€ ë¶„ë¥˜ $class$ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ë¹„ìœ¨ì´ë‹¤. ì„ì˜ì˜ ë‹¨ì–´ $w_i$ì— ëŒ€í•´ $ratio(w_i)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$
ratio(w_i) = \frac{P(w_i|Pos)}{P(w_i|Neg)}
$$

Likelihoodë€ í‘œë³¸ì„ ê²°í•© í™•ë¥ ë¡œ ë‚˜íƒ€ë‚¸ í•¨ìˆ˜ì´ë©°, ì—¬ê¸°ì„œëŠ” ì…ë ¥ ë¬¸ì¥$s$ê°€ ì„ì˜ì˜ $class$ì¼ í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. ì—¬ê¸°ì„œëŠ” Likelihoodë¥¼ ratioì— ëŒ€í•´ ì •ì˜í•˜ì. ì¦‰ ëª¨ë“  ì…ë ¥ê°’ $w_i \in s$ì— ëŒ€í•´ ratioë¥¼ ê³±í•œ ê°’ìœ¼ë¡œ í‘œí˜„í•œë‹¤.

$$
likelihood(s) = \prod^{m}_{i=1}\frac{P(w_i|Pos)}{P(w_i|Neg)}
$$

ë§Œì•½ ì…ë ¥ê°’ì˜ ëª¨ë“  ë‹¨ì–´ $w_i$ê°€ corpusì˜ ê¸ì •ì ì¸ ë¼ë²¨ê³¼ ë¶€ì •ì ì¸ ë¼ë²¨ì—ì„œ ê°™ì€ ë¹ˆë„ë¡œ ë‚˜íƒ€ë‚¬ë‹¤ë©´ likelihood ê°’ì€ `1`ë¡œ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤. ì´ ê²°ê³¼ë¥¼ ê¸ì •ì ì´ì§€ë„ ë¶€ì •ì ì´ì§€ë„ ì•Šì€ **ì¤‘ë¦½ ê°’**ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë°˜ë©´ ë¶„ëª¨ ë¶„ìëŠ” ë¹ˆë„ ìˆ˜ì´ë¯€ë¡œ likelihoodëŠ” ìŒì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ì—†ê³ , ë¶„ëª¨ $P(w_i|Neg)$ê°€ ë¶„ì $P(w_i|Pos)$ ë³´ë‹¤ ì»¤ì§ˆ ìˆ˜ë¡ 0ì— ê°€ê¹Œì›Œì§€ê³  ë°˜ëŒ€ì˜ ê²½ìš° ì–‘ì˜ ë¬´í•œëŒ€ ê°’ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ ìˆë‹¤.

### ğŸ² Naive ë€?
ë² ì´ì¦ˆ ëª¨ë¸ì´ **naive**(ìˆœì§„í•˜ë‹¤)ëŠ” ë§ì€ ëª¨ìˆ˜ì˜ ëª¨ë“  í‘œë³¸ì´ ìƒí˜¸ ë…ë¦½ì ì´ê³  ì™„ì „í•˜ë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒì„ ëœ»í•œë‹¤. ì¦‰ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì—ì„œëŠ” ë°ì´í„°ì˜ ëª¨ë“  íŠ¹ì„±ë“¤(features)ì„ ì•Œ ìˆ˜ ìˆê³ , ë‚˜ì•„ê°€ íŠ¹ì„±ë“¤ì´ ì„œë¡œ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•˜ëŠ” ê²ƒì„ ëœ»í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í•œ ë¬¸ì¥ì„ ë°ì´í„° í•œê°œë¼ê³  í•˜ë©´, ë¬¸ì¥ì— ì†í•œ ë‹¨ì–´ë¥¼ ë°ì´í„°ì˜ íŠ¹ì„±ë“¤ë¡œ ë³¼ ìˆ˜ ìˆê³  ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì€ ì´ ë‹¨ì–´ë“¤ì´ ìƒí˜¸ ì—°ê´€(covariate) ë˜ì–´ìˆì§€ ì•Šë‹¤ê³  ê°€ì •í•œë‹¤. 

ë‹¨ì–´ $w_i (i= 1, ..., n)$ì„ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì´ classì— ì†í•  í™•ë¥ ì€ ê²°í•© í™•ë¥  $P(class, w_1, ..., w_n)$ì¸ë°, ì—°ì‡„ ë²•ì¹™ì— ë”°ë¥´ë©´:

$$
\begin{aligned}
P(class, w_1, ..., w_n) 
&= P(class) \cdot P(w_1, ..., w_n) \\
&= P(class) \cdot P(w_1|class) \cdot P(w_2, ..., w_n) \\
&= P(class) \cdot P(w_1|class) \cdot P(w_2|class, w_1) \cdot P(w_3, ..., w_n) \\
&= ...
\end{aligned} 
$$

ì´ë ‡ê²Œ ë¬¸ì¥ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ì•ì— ë“±ì¥í•œ ë‹¨ì–´ë“¤ê³¼ classì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥  ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ë‚˜ì´ë¸Œ ê°€ì •ì€ íŠ¹ì„±ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•˜ë¯€ë¡œ ì„ì˜ì˜ ìŒ $i \neq j$ì— ëŒ€í•´ $P(w_i|class) = P(w_i|class, w_j)$ë¥¼ ë§Œì¡±í•œë‹¤. ë”°ë¼ì„œ ë¬¸ì¥ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë³´ë‹¤ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
P(class, w_1, ..., w_n) 
&= P(class) \cdot P(w_1|class) \cdot ... \cdot P(w_n|class) \\
&= P(class) \cdot \prod_{i=1}^{n} P(w_i|class)
\end{aligned} 
$$

í˜„ì‹¤ ì„¸ê³„ì˜ ë§ì€ í˜„ìƒì´ ìƒí˜¸ ì˜ì¡´ì ì„ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì€ ì˜¤ë«ë™ì•ˆ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ëª¨ë¸ë¡œ í™œìš©ë˜ì–´ì™”ë‹¤.

> [ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì˜ íš¨ê³¼ ë¶„ì„ ìë£Œ ë³´ê¸°](https://web.archive.org/web/20171210133853/http://www.research.ibm.com/people/r/rish/papers/RC22230.pdf)

### ğŸ² ë¡œê·¸ê°’ìœ¼ë¡œ ê³„ì‚°í•˜ê¸°
í™•ë¥ ì€ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì´ë¯€ë¡œ, í™•ë¥ ì„ ì—¬ëŸ¬ë²ˆ ê³±í•˜ë©´ ì „ì‚°ì ìœ¼ë¡œ ì–¸ë”í”Œë¡œìš°ì˜ ìœ„í—˜ì´ ì»¤ì§„ë‹¤. ë„ˆë¬´ í° ê°’ì´ë‚˜ ë„ˆë¬´ ì‘ì€ ê°’ì„ ë‹¤ë£¨ëŠ” ì „í˜•ì ì¸ ë°©ë²•ì€ ë¡œê·¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ë¡œê·¸ë¥¼ ì·¨í•œ log likelihoodëŠ” log ratioì˜ í•©ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
log\_ ratio(w_i) &= log \frac{P(w_i|Pos)}{P(w_i|Neg)} \\
log\_ likelihood &= \sum^{m}_{i=1} log \frac{P(w_i|Pos)}{P(w_i|Neg)}
\end{aligned}
$$

í•œê°€ì§€ ê°œë…ì„ ì¶”ê°€í•˜ìë©´, log ratioì˜ ê°’ì„ lambda í•¨ìˆ˜ë¡œ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤. ì¦‰ $\lambda(w)$ë¥¼ log ratioë¡œ í‘œí˜„í•˜ë©´ likelihoodë¥¼ ë” ê°„ë‹¨í•˜ê²Œ ì“¸ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
\lambda(w_i) &= log\frac{P(w_i|Pos)}{P(w_i|Neg)} \\
log\_ likelihood(s) &= \sum^{n}_{i=1} \lambda(w_i)
\end{aligned}
$$

ë¡œê·¸ë¥¼ ì·¨í•˜ê²Œ ë˜ë©´ likelihoodì˜ ì¤‘ë¦½ ê°’ì€ $1$ì—ì„œ $log 1 = 0$ìœ¼ë¡œ ë³€í•˜ê²Œ ëœë‹¤. $0$ì—ì„œ ì–‘ì˜ ë¬´í•œëŒ€ì— ëŒ€í•œ ë¡œê·¸ê°’ì€ ìŒì˜ ë¬´í•œëŒ€ì—ì„œ ì–‘ì˜ ë¬´í•œëŒ€ì´ë¯€ë¡œ log likelihoodì˜ ê°’ì˜ ë²”ìœ„ë„ $0$ì„ ì¤‘ë¦½ ê°’ìœ¼ë¡œí•˜ëŠ” ìŒì˜ ë¬´í•œëŒ€ì—ì„œ ì–‘ì˜ ë¬´í•œëŒ€ ê°’ì„ ë°˜í™˜í•  ê²ƒì´ë‹¤.

### ğŸ² ì‚¬ì „í™•ë¥ 
ì˜ˆë¥¼ ë“¤ì–´ ì½”ë¡œë‚˜ íŒ¬ë°ë¯¹ì— ëŒ€í•œ íŠ¸ìœ—ì„ ëª¨ì•„ì„œ ì •ì„œ ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤ë©´, ë¶€ì •ì ì¸ íŠ¸ìœ—ì´ ê¸ì •ì ì¸ íŠ¸ìœ—ë³´ë‹¤ ë§ì„ ê²ƒì´ë‹¤. í˜„ì‹¤ ë°ì´í„° corpusì—ì„œ ë¶„ë¥˜ í´ë˜ìŠ¤ê°€ ê· ë“±í•˜ê²Œ ë‚˜ëˆ ì§€ëŠ” ê²½ìš°ëŠ” ë“œë¬¼ê¸° ë•Œë¬¸ì—, ë°ì´í„°ì˜ ë¶ˆê· í˜•ì„ ë³´ì •í•˜ê¸° ìœ„í•œ ì ˆì°¨ê°€ í•„ìš”í•˜ë‹¤. 

ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì—ì„œëŠ” ì‚¬ì „í™•ë¥ ì´ ì´ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. Likelihoodì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì‚¬ì „í™•ë¥ ì„ classì˜ ë¹„ìœ¨ë¡œ ì •ì˜í•˜ê³  ë¡œê·¸ê°’ì„ ì·¨í•  ìˆ˜ ìˆë‹¤. 

$$
log\_prior = log \frac{P(Pos)}{P(Neg)}
$$

ì–´ë–¤ ì…ë ¥ê°’ì— ëŒ€í•´ log likelihoodê°€ 0ì´ë¼ê³  í•˜ë©´, ì˜ˆì¸¡ê°’ì€ log priorì™€ ê±°ì˜ ê°™ì„ ê²ƒì´ë‹¤. 

## 4. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸
ë‹¤ì‹œ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì—°ì‚°ì„ ë³´ë©´, ì…ë ¥ ë¬¸ì¥ $s$ì— ëŒ€í•´:

$$
NB = log\frac{P(Pos)}{P(Neg)} + \sum^{n}_{i=1} \lambda(w_i)
$$

ìœ„ì˜ ì‹ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, 

$$
NB = log\_prior + log\_likelihood
$$

ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì˜ ì—°ì‚°ì˜ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤. 

0. í›ˆë ¨ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•œë‹¤.
1. í† í°í™” ëœ ë‹¨ì–´ì˜ ë¹ˆë„ $freq(w, class)$ë¥¼ ê³„ì‚°í•œë‹¤.
2. ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì˜ ë‹¨ì–´ì— ëŒ€í•´ í›ˆë ¨í•´ log priorì™€ log likelihood ê°’ì„ êµ¬í•œë‹¤.
    - ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì˜ ë‹¨ì–´ì— ëŒ€í•´ $P(w|Pos)$ì™€ $P(w|Neg)$ ê°’ì„ êµ¬í•œë‹¤.
    - ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì˜ ë‹¨ì–´ì— ëŒ€í•´ $P(Pos)$ì™€ $P(Neg)$ ê°’ì„ êµ¬í•œë‹¤.
3. í›ˆë ¨í•œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¡œ ì •ì„œë¥¼ ë¶„ë¥˜í•œë‹¤.


### ğŸ“‚ êµ¬í˜„í•˜ê¸°

#### 1. í† í°í™” ëœ ë‹¨ì–´ì˜ ë¹ˆë„ $freq(w, class)$ë¥¼ ê³„ì‚°í•œë‹¤.
```python
def get_freq(dd, train_x, train_y):
    '''
    Get frequency dictionary from the training data.
    input:
        dd : a defaultdict of integer.
        train_x : list of tokened sentences of training data.
        train_y : list of 0 or 1 corresponding to the train_x. 
    return:
        result : dictionary of (key, value) = (word label pair, frequency).
    '''
    for label, sentence in zip(train_y, train_x):
        for word in process(sentence):
            dd[(word, label)] += 1

    return dd

# count frequency dictionary from train_x and train_y.
freqs = get_freq(defaultdict(int), train_x, train_y)
```

#### 2. ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì˜ ë‹¨ì–´ì— ëŒ€í•´ í›ˆë ¨í•´ log priorì™€ log likelihood ê°’ì„ êµ¬í•œë‹¤.
```python
def train_naive_bayes(freqs, train_x, train_y):
    '''
    Train Naive Bayes model, that is, get prior and likelihood from the training data.
    return:
        log_prior : an integer. P(Pos) / P(Neg) value.
        log_likelihood : a dictionary of (key, value) = (word, log likelihood)
    '''
    # log_likelihood relies on words
    log_likelihood = {}
    # log prior value relies on the corpus
    log_prior = 0

    # get unique words from the frequency dict
    vocab = list(set(freq.keys()))
    V = len(vocab)

    # get N_pos and N_neg
    N_pos = N_neg = 0
    for pair in freqs.keys():
        # if label is 1(> 0), the word is positive.
        if pair[1] > 0:
            N_pos += freqs[pair]
        # if label is 0, the word is negative.
        else:
            N_neg += freqs[pair]

    # get log likelihood
    for w in vocab:
        # get positive and negative frequency of word w.
        freq_pos = freqs.get((w, 1), 0)
        freq_neg = freqs.get((w, 0), 0)

        # get P(w|Pos) and P(w|Neg).
        p_w_pos = (freq_pos + 1) / (N_pos + V)
        p_w_neg = (freq_neg + 1) / (N_neg + V)

        log_likelihood[w] = np.log(p_w_pos) - np.log(p_w_neg)

    # to compute log_prior,
    # get the number of positive and negative labels
    num_label = len(train_y)
    num_pos = len(train_y[train_y == 1])
    num_neg = len(train_y[train_y == 0])

    # log prior = log(P(Pos)) - log(P(Neg))
    log_prior = np.log(num_pos / num_label) - np.log(num_neg / num_label)

    return log_prior, log_likelihood

# get log prior and log likelihood from the training data
# so that we can train on test data.
log_prior, log_likelihood = train_naive_bayes(freqs, train_x, train_y)
```

#### 3. í›ˆë ¨í•œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¡œ ì •ì„œë¥¼ ë¶„ë¥˜í•œë‹¤.
```python
def predict_naive_bayes(s, log_prior, log_likelihood):
    '''
    input:
        s : a list. Input sentence.
        log_prior : log prior from trained naive bayes.
        log_likelihood : log likelihood from trained naive bayes.
    return:
        log_prob : float between 0 and 1. probability that s is positive.
    '''    
    
    words = proprocess(s)

    log_prob = 0

    for w in words:
        if w in log_likelihood:
            log_prob += log_likelihood[w]

    log_prob += log_prior

    return log_prob

# print probability of test data.
test_data = 'hope you get well soon. it hurts to see you ill ğŸ˜¢'
print('prediction:', predict_naive_bayes(test_data, log_prior, log_likelihood))

# output: 3.5905424260671044 -- ê¸ì • ì •ì„œë¡œ ì˜ˆì¸¡í–ˆë‹¤.
```


## ì°¸ê³  ìë£Œ
1. Coursera, deeplearning.ai, Natural Language Processing with Classification and Vector Spaces, week 2 
2. Wikipedia, Naive Bayes Classification, https://en.wikipedia.org/wiki/Naive_Bayes_classifier
3. Wikipedia, Additive Smooothing, https://en.wikipedia.org/wiki/Additive_smoothing
4. Wikipedia, Likelihood-ratio test, https://en.wikipedia.org/wiki/Likelihood-ratio_test