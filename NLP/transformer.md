
ì› ë…¼ë¬¸:
> Vaswani et al., 2017, "Attention is all you need" ([Link to arxiv](https://arxiv.org/abs/1706.03762))

## 0. Transformer

 íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” Seq2Seq ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ê°–ê³  ìˆì§€ë§Œ, ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£°ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ í™˜ì˜ë°›ì•˜ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ìƒˆë¡œìš´ ì–´í…ì…˜ì„ ë„ì…í–ˆë‹¤. 2014ë…„ì— ë“±ì¥í•œ ì–´í…ì…˜(Bahdanau et al., 2014)ì´ RNN ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í™œìš©ëœ ê²ƒê³¼ ë‹¬ë¦¬, 2017ë…„ì˜ ì–´í…ì…˜ì€ ì‹ ê²½ë§ì„ ì´ìš©í•˜ì§€ ì•Šê³  í–‰ë ¬ ê³±ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°©ì‹ì„ ì œì•ˆí•˜ë©´ì„œ ìì—°ì–´ ì²˜ë¦¬ì— ìˆì–´ íšê¸°ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ë¶ˆëŸ¬ì™”ë‹¤. 

![](img/transformer.png)
<center>
íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ êµ¬ì¡°, Vaswani et al., 2017
</center>

</br>

íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ í¬ê²Œ ë´ì„œ ì™¼ìª½ì˜ ì¸ì½”ë”ì™€ ì˜¤ë¥¸ìª½ì˜ ë””ì½”ë”ë¥¼ ê°€ì§€ëŠ” êµ¬ì¡°ì´ë‹¤. RNN ëª¨ë¸ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¸ì½”ë”ì˜ ê²°ê³¼ê°’ì„ ë””ì½”ë”ì˜ ì…ë ¥ê°’ê³¼ ê²°í•©í•´ì„œ ì—°ì‚°ì„ ê±°ì³ ê²°ê³¼ê°’ì„ ë„ì¶œí•œë‹¤. ë‹¤ë§Œ RNNê³¼ì˜ ì°¨ì´ì ì€ ì‹œí€€ìŠ¤ë¥¼ **ë™ì‹œì—** ì²˜ë¦¬í•œë‹¤ëŠ” ì ì´ë‹¤. ì¦‰ ì‹œí€€ìŠ¤ ëª¨ë¸ì²˜ëŸ¼ ì‹œí€€ìŠ¤ ë°ì´í„°ì— ìˆœì„œëŒ€ë¡œ ì ‘ê·¼í•´ì„œ í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ì—°ì‚°í•œë‹¤. ë”°ë¼ì„œ ë¶„ì‚° ì—°ì‚°ì´ ìš©ì´í•˜ë©° RNN ëª¨ë¸ì—ì„œ ë°œìƒí•˜ëŠ” vanishing gradient í˜„ìƒì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì²˜í•´ì„œ ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ ë„¤íŠ¸ì›Œí¬ì— ì‚¬ìš©ë˜ëŠ” ì–´í…ì…˜ì€ í–‰ë ¬ê³±ì„ ì´ìš©í•œ ìˆœì°¨ì ì´ì§€ ì•Šì€ (ë”°ë¼ì„œ ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•œ) ì–´í…ì…˜ì´ë©°, ì´ ì–´í…ì…˜ì„ Scaled-Dot product ì–´í…ì…˜ì´ë¼ê³  í•œë‹¤.  

### ğŸ”” Scaled-Dot prooduct Attention

Scaled-Dot prooduct ì–´í…ì…˜ì—ëŠ” ì„¸ê°œì˜ í–‰ë ¬ $Q$, $K$, $V$ ì…ë ¥ì´ í•„ìš”í•˜ë‹¤:

- $Q$, Queries : ë¹„êµí•˜ê³ ì í•˜ëŠ” ì‹œí€€ìŠ¤ë¡œ, í‚¤ Kì™€ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•œë‹¤.
- $K$, Keys : ë¹„êµ ëŒ€ìƒì´ ë˜ëŠ” ì‹œí€€ìŠ¤ë¡œ, ì¿¼ë¦¬ Qì™€ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•œë‹¤.
- $V$, Values : $QK$ì˜ ê°€ì¤‘ì¹˜ê°€ ê³±í•´ì§€ëŠ” í–‰ë ¬ì´ë‹¤.

ì–´í…ì…˜ì´ë€ í–‰ë ¬ $Q$ì™€ $K$-$V$ ìŒì„ ê²°ê³¼ê°’ì— ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ë¡œ, ê²°ê³¼ê°’ì€ $V$ì˜ ê°€ì¤‘ì¹˜ í•©ìœ¼ë¡œ ê³„ì‚°ë˜ê³ , ê°ê°ì˜ ê°’ $v$ì— í• ë‹¹ëœ ê°€ì¤‘ì¹˜ëŠ” $q$ì™€ ê·¸ì— ëŒ€ì‘í•˜ëŠ” $k$ë¡œë¶€í„° ê³„ì‚°ëœë‹¤. ì´ ì–´í…ì…˜ì´ ì´ë¦„ ë¶™ì€ ì´ìœ ëŠ” ì–´í…ì…˜ì„ ì—°ì‚°í•˜ëŠ” ë°©ë²•ì— ìˆë‹¤.

$$
Attention(Q, K, V) = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V
$$

ì²« ë²ˆì§¸ í–‰ë ¬ ê³±(dot product)ì€ ì¿¼ë¦¬ Qì™€ í‚¤ Kì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³ , ë‘ë²ˆì§¸ ê³±ì€ softmax í•¨ìˆ˜ê°’ìœ¼ë¡œ ì–»ì–´ì§„ ê°€ì¤‘ì¹˜ë¥¼ ë°œë¥˜ $V$ì— í• ë‹¹í•œë‹¤. $Q$ì™€ $K$ì˜ ê³±ì€ $K$ì˜ ì°¨ì› $d_k$ì˜ ë£¨íŠ¸ë¡œ ì¡°ì •(scaled) í•˜ëŠ”ë°, ì´ ìŠ¤ì¼€ì¼ë§ì€ reguralizationíš¨ê³¼ë¥¼ ê°€ì§„ë‹¤. ê·¸ ë‹¤ìŒ softmax í•¨ìˆ˜ë¥¼ ê±°ì³ $Q$ì™€ $K$ì˜ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜í•´ ë§ˆì§€ë§‰ìœ¼ë¡œ ê·¸ ê°’ì„ í–‰ë ¬ $V$ì— ë°˜ì˜í•´ ëª¨ë“  ì¿¼ë¦¬ì˜ ì–´í…ì…˜ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. 

![](img/attention.png)
<center>
ì–´í…ì…˜, Badnau et al., 2015
</center>

</br>

ì–´í…ì…˜ì€ ì¿¼ë¦¬ì™€ í‚¤ì˜ heatmapì„ ìƒì„±í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ë‹¨ì–´ ìŒ ë‹¨ìœ„ $(q, k)$ë¡œ ì‚´í´ë³´ê¸° ë•Œë¬¸ì—, ë‹¨ì–´ê°€ ë¬¸ì¥ì—ì„œ ë“±ì¥í•˜ëŠ” ìˆœì„œì— ê´€ê³„ì—†ì´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ì˜ˆì²˜ëŸ¼ ì–´ìˆœì´ ë‹¤ë¥¸ ì˜ì–´ì™€ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì—ì„œë„ ë‹¨ì–´ì˜ ëŒ€ì‘ ê´€ê³„ë¥¼ ì œëŒ€ë¡œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

### ğŸ”” Multi-Head Attention

![](img/multi-head-attention.png)
<center>
Multi-Head Attention, from deeplearning.ai
</center>

</br>

íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì–´í…ì…˜ì˜ ë˜ë‹¤ë¥¸ íŠ¹ì§•ì€ ì–´í…ì…˜ì´ ë¨¸ë¦¬(heads)ë¥¼ ì—¬ëŸ¬ê°œ ê°€ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ì…ë ¥ê°’ì— ëŒ€í•´ ì–´í…ì…˜ì„ ì—¬ëŸ¬ë²ˆ ìˆ˜í–‰í•œ í›„ ê²°ê³¼ë¥¼ ê²°í•©(concatenate)í•´ì„œ ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ í–‰ë ¬ì„ ë§Œë“œëŠ” ê²ƒì„ ëœ»í•œë‹¤. $d_{model}$ ì°¨ì›ì˜ $Q$, $K$, $V$ì— ëŒ€í•´ ì¼íšŒ ì–´í…ì…˜ì„ ì ìš©í•˜ëŠ” ëŒ€ì‹  $Q$, $K$, $V$ë¥¼ `n_heads`ë²ˆ ì„ í˜• íˆ¬ì‚¬(linear projection)í•´ì„œ ì„œë¡œë‹¤ë¥¸ í•™ìŠµëœ ì„ í˜• íˆ¬ì‚¬ë“¤ì— ëŒ€í•´ ì–´í…ì…˜ì„ ì ìš©í•œë‹¤. ê° ì„ í˜• íˆ¬ì‚¬ë¥¼ $Q$ì— ëŒ€í•´ $W^Q$, $K$ì— ëŒ€í•´ $W^K$, $V$ì— ëŒ€í•´ $W^V$ë¼ê³  í•˜ë©° ê°ê° $(n_{seq}, d_k)$, $(n_{seq}, d_k)$, $(n_{seq}, d_v)$ ì°¨ì›ì„ ê°€ì§„ë‹¤. ì–´í…ì…˜ì„ ì ìš©í•˜ë©´ í•œê°œì˜ ê²°ê³¼ê°’ì— ëŒ€í•´ $(n_{seq}, d_v)$ ì°¨ì›ì„ ê°€ì§€ë¯€ë¡œ `n_heads`ê°œì˜ ê²°ê³¼ê°’ì„ ê²°í•©í•œ í–‰ë ¬ ë‹¤ì‹œ ì„ í˜• íˆ¬ì‚¬í•´ì„œ ì–´í…ì…˜ ê²°ê³¼ê°’ì„ ì–»ëŠ”ë‹¤. ë§¤ë²ˆ í•™ìŠµí•  ë•Œë§ˆë‹¤ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ë¯€ë¡œ ì—¬ëŸ¬ë²ˆ ì–´í…ì…˜ì„ ì ìš©í•¨ìœ¼ë¡œì„œ ì‹œí€€ìŠ¤ ë°ì´í„° ì‚¬ì´ì˜ ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ì´ë•Œ ì—¬ëŸ¬ë²ˆì˜ ì–´í…ì…˜ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì•¼ í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ë³‘ë ¬ ì—°ì‚°ì„ í•˜ê¸° ìš©ì´í•œ êµ¬ì¡°ë‹¤. 

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax `SplitIntoHeads`ì™€ `MergeHeads` ë©”ì„œë“œë¡œ, Multi-Head ì–´í…ì…˜ì˜ ì¼ë¶€ë¶„ì„ êµ¬í˜„í•œë‹¤. 

```python
def SplitIntoHeads(n_heads, merged_batch_and_head=True):
  """Returns a layer that reshapes an array for multi-head computation."""
  def f(x):
    batch_size, seq_len, d_feature = x.shape
    if d_feature % n_heads != 0:
      raise ValueError(
          f'Feature embedding dimensionality ({d_feature}) is not a multiple'
          f' of the requested number of attention heads ({n_heads}).')

    d_head = d_feature // n_heads

    # (b_size, seq_len, d_feature) --> (b_size*n_heads, seq_len, d_head)
    x = x.reshape((batch_size, seq_len, n_heads, d_head))
    x = x.transpose((0, 2, 1, 3))
    if merged_batch_and_head:
      x = x.reshape((batch_size * n_heads, seq_len, d_head))
    return x
  return Fn('SplitIntoHeads', f)
```

`SplitIntoHeads`ëŠ” `d_feature`(ê·¸ë¦¼ì—ì„œëŠ” `d_model`)ê°€ Headì˜ ê°œìˆ˜ `n_heads`ì˜ ì •ìˆ˜ë°°ì¼ ê²ƒì„ ê°•ì œí•œë‹¤. ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ `(batch, seq_len, d_feature)`ì„ ì…ë ¥ë°›ì•„ `(b_size*n_heads, seq_len, d_head)`ì°¨ì›ì„ ì¶œë ¥í•˜ê³  ìˆë‹¤. ê·¸ ë‹¤ìŒ `PureAttention`ì²˜ëŸ¼ 1ê°œ Headì— ëŒ€í•œ ì–´í…ì…˜ì„ `n_heads`ë§Œí¼ ìˆ˜í–‰í•œ í›„, ì—¬ëŸ¬ Headë¥¼ `MergeHeads`ë¡œ ê²°í•©í•œë‹¤.

```python
def MergeHeads(n_heads, merged_batch_and_head=True):
  """Returns a layer that rejoins heads, after multi-head computation."""
  def f(x):
    if merged_batch_and_head:
      dim_0, seq_len, d_head = x.shape
      if dim_0 % n_heads != 0:
        raise ValueError(
            f"Array's leading dimension ({dim_0}) is not a multiple of the"
            f" number of attention heads ({n_heads}).")

      batch_size = dim_0 // n_heads
      x = x.reshape((batch_size, n_heads, seq_len, d_head))
    else:
      batch_size, _, seq_len, d_head = x.shape

    # (b_size, n_heads, seq_len, d_head) --> (b_size, seq_len, d_feature)
    x = x.transpose((0, 2, 1, 3))
    x = x.reshape((batch_size, seq_len, n_heads * d_head))
    return x
  return Fn('MergeHeads', f)
```

í•¨ìˆ˜ `f`ëŠ” `(batch_size, n_heads, seq_len, d_head)` ì°¨ì›ì˜ ì…ë ¥ê°’ì„ ì–´í…ì…˜ ë¸”ëŸ­ì˜ ì…ë ¥ê°’ ì°¨ì› `(b_size, seq_len, d_feature)`ë¡œ ë³€í˜•í•´ì„œ ë°˜í™˜í•œë‹¤. 


## 1. Encoder

ë„ë¦¬ ì“°ì´ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì¸ BERTì˜ ê²½ìš° ì¸ì½”ë”ë§Œ (`bert-large`ì˜  ê²½ìš°) 24ê°œ ìŒ“ì€ ë„¤íŠ¸ì›Œí¬ë‹¤. LSTMì„ ìƒê°í•´ ë³´ë©´ ì¸ì½”ë”ì˜ êµ¬ì¡°ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‹¨ìˆœí•´ ë³´ì´ì§€ë§Œ, ì¸ì½”ë”ë§Œìœ¼ë¡œë„ ë§ì€ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![](img/transformer_encoder.png)
<center>
ì¸ì½”ë” êµ¬ì¡°, from deeplearning.ai
</center>

</br>

ì¸ì½”ë”ëŠ” í¬ê²Œ **ë‘ê°œì˜ ë ˆì´ì–´**ë¡œ êµ¬ì„±ëœë‹¤. Multi-Head Attentionê³¼ FeedForwardì´ë‹¤. FeedForwardëŠ” ì¼ë ¨ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„±ëœ ë¸”ëŸ­ì´ë‹¤. 0ï¸âƒ£ ì…ë ¥ê°’ì„ Embeddingí•˜ê³  Positional Encodingì„ ì ìš©í•œ í›„ì—, í•œ ê°œì˜ **ì¸ì½”ë” ë¸”ëŸ­**ì€ 1ï¸âƒ£ Residualì„ ì ìš©í•œ Multi-Head Attentionì„ ì‹¤í–‰í•˜ê³  2ï¸âƒ£ ë‹¤ì‹œ Residualì„ ì ìš©í•œ FeedForward ë ˆì´ì–´ë¥¼ ì‹¤í–‰í•œë‹¤. ëª¨ë¸ì„ ê¹Šê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì´ ì¸ì½”ë” ë¸”ëŸ­ì„ ì—¬ëŸ¬ë²ˆ ì‹¤í–‰í•œë‹¤.

Residual ë ˆì´ì–´ëŠ” í•¨ìˆ˜ $Fn(x_1, x_2, ...)$ì— ëŒ€í•´ ë‹¤ìŒì„ ëœ»í•œë‹¤.

$$
Residual(Fn)(x_1, x_2, ...) = Fn(x_1, x_2, ...) + x_1
$$

ì¦‰ ì…ë ¥ê°’ì˜ ì²«ë²ˆì§¸ ê°’ì„ í•¨ìˆ˜ì˜ ê²°ê³¼ê°’ì— ë”í•˜ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. Residual ë ˆì´ì–´ë¥¼ ì ìš©í•˜ëŠ” ì´ìœ ëŠ” shortcut ì—°ê²° ê¸°ëŠ¥ì„ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. íŠ¹íˆ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•  ë•Œ Residualì´ íš¨ê³¼ì ì„ì´ ê²€ì¦ë˜ì—ˆë‹¤.

> Further study - Residual ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  : [He et al., 2015](https://arxiv.org/pdf/1512.03385.pdf)

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `TransformerEncoder` ëª¨ë¸ì´ë‹¤.

```python
def TransformerEncoder(vocab_size,
                       n_classes=10,
                       d_model=D_MODEL,
                       d_ff=D_FF,
                       n_layers=N_LAYERS,
                       n_heads=N_HEADS,
                       max_len=MAX_SEQUENCE_LENGTH,
                       dropout=DROPOUT_RATE,
                       dropout_shared_axes=DROPOUT_SHARED_AXES,
                       mode=MODE,
                       ff_activation=FF_ACTIVATION_TYPE):
    
  def _Dropout():
    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)

  def _EncBlock():
    return _EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,
                         mode, ff_activation)

  return tl.Serial(
      tl.Branch([], tl.PaddingMask()),  # Creates masks from copy of the tokens.
      tl.Embedding(vocab_size, d_model),
      _Dropout(),
      tl.PositionalEncoding(max_len=max_len),
      [_EncBlock() for _ in range(n_layers)],
      tl.Select([0], n_in=2),  # Drops the masks.
      tl.LayerNorm(),
      tl.Mean(axis=1),
      tl.Dense(n_classes),
  )
```

`TransformerEncoder`ëŠ” í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ `n_classes`ê°œë¡œ ë¶„ë¥˜í•œë‹¤. í•¨ìˆ˜ ë°˜í™˜ê°’ì˜ ì²«ì¤„ì— ë“±ì¥í•˜ëŠ” `tl.Branch`ëŠ” ì…ë ¥ê°’ì„ ë°›ì•„ì„œ ê°ê°ì˜ í•¨ìˆ˜ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ì‹¤í–‰í•œë‹¤. ì¦‰ ì…ë ¥ê°’ì„ ë¦¬ìŠ¤íŠ¸`[]`ë¡œ ë§Œë“  ê°’ê³¼ íŒ¨ë”©ë§ˆìŠ¤í¬ `tl.PaddingMask()` ê°’ ë‘ê°œë¥¼ ë°˜í™˜í•  ê²ƒì´ë‹¤. ë‘ ê°’ ëª¨ë‘ ì„ë² ë”©ê³¼ positional encodingì„ ê±°ì³ ì¸ì½”ë” ë¸”ëŸ­ì— ì…ë ¥ëœë‹¤. ì¸ì½”ë” ë¸”ëŸ­ì˜ ì½”ë“œì—ì„œ ë°ì´í„°ì™€ ë§ˆìŠ¤í¬ ìŒ `(activations, mask)`ì„ ì…ë ¥ë°›ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
def _EncoderBlock(d_model,
                  d_ff,
                  n_heads,
                  dropout,
                  dropout_shared_axes,
                  mode,
                  ff_activation):
  """Returns a list of layers that implements a Transformer encoder block.
  The input to the block is a pair (activations, mask) where the mask was
  created from the original source tokens to prevent attending to the padding
  part of the input. The block's outputs are the same type/shape as its inputs,
  so that multiple blocks can be chained together.
  """
  def _Attention():
    return tl.Attention(d_model, n_heads=n_heads, dropout=dropout, mode=mode)

  # ...

  return [
      tl.Residual(
          tl.LayerNorm(),
          _Attention(),
          _Dropout(),
      ),
      tl.Residual(
          tl.LayerNorm(),
          _FFBlock(),
          _Dropout(),
      ),
  ]
```

ì—¬ê¸°ì„œ `tl.Attention`ì€ `n_heads`ê°œì˜ ë¨¸ë¦¬ë¥¼ ê°€ì§€ëŠ” multi-head ì…€í”„-ì–´í…ì…˜ì´ë©°, Attention ë ˆì´ì–´ì™€ FeedForward ë¸”ëŸ­ì´ Residualì„ ê±°ì¹˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì–´í…ì…˜ ë¸”ëŸ­ì„ ì§€ë‚œ í›„ì—ëŠ” ë§ˆìŠ¤í¬ê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ `tl.Select()`ë¡œ `(activations, mask)`ìŒì—ì„œ ì•ì˜ ê°’ë§Œ ì·¨í•˜ê³  `tl.LayerNorm()`ê³¼ ê°™ì€ ì¿¼ë¦¬ì— í•´ë‹¹í•˜ëŠ” ì—´ì— ëŒ€í•œ ë§ì…ˆ `tl.Mean(axis=1)`, ê·¸ë¦¬ê³  `n_classes`ê°œì˜ `tl.Dense()` ì¸µì„ ê±°ì³ ë§ˆë¬´ë¦¬í•œë‹¤.

### ğŸ”† Dimensionality Setting

Residual ë ˆì´ì–´ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´í…ì…˜ì˜ ì…ë ¥ê°’ê³¼ ê²°ê³¼ê°’ì˜ ì°¨ì›ì´ ê°™ì•„ì•¼í•œë‹¤. ì•ì˜ Multi-Head Attention ê·¸ë¦¼ì„ ì°¸ê³ í•˜ì—¬ ë°°ì¹˜ í¬ê¸°ë¥¼ `batch`, ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ `length`, ê·¸ë¦¬ê³  ì–´í…ì…˜ì˜ ì°¨ì›ì„ `d_model`ë¡œ ì„¤ì •í•˜ì. Q, K, Vê°€ $(batch, length, d_{model})$ ì°¨ì›ì„ ê°€ì§€ë©°, $W^Q$, $W^K$, $W^V$ëŠ” ê°ê° $(batch, length, d_k)$, $(batch, length, d_k)$, $(batch, length, d_v)$ ì°¨ì›ì´ë¼ê³  í•˜ì. ìœ„ì˜ ì„¤ì •ì—ì„œ $d_k = d_v$ë¡œ ë‘ë©°, ê·¸ë¦¼ì—ì„œëŠ” ì´ ê°’ì´ $d_{head}$ë¡œ ë‚˜íƒ€ë‚˜ ìˆë‹¤. ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ê³  ë‚œ í›„ ië²ˆ ì§¸ ì–´í…ì…˜ì€ $Z_i \in (batch, length, d_v)$ ì°¨ì›ì´ ë˜ëŠ”ë°, $n_{heads}$ê°œì˜ ì–´í…ì…˜ì„ ê²°í•©í•˜ê³  ë‚œ í›„ ì²˜ìŒ ì…ë ¥ê°’ê³¼ ê°™ì€ ì°¨ì›ì„ ì–»ê¸° ìœ„í•´ $ n_{heads} = d_{model} / d_v$ë¡œ ì„¤ì •í•œë‹¤. ìš”ì•½í•˜ë©´:

$$
d_k = d_v = d_{model} / n_{heads}
$$


### ğŸ”† Positional Encoding

ì¸ì½”ë”©ì— ì•ì„œ, ë‹¨ìˆœíˆ ë‹¨ì–´ ì„ë² ë”©ì„ í†µí•´ $QK^T$ 2ì°¨ì› í–‰ë ¬ì„ ê³„ì‚°í•˜ë©´ ì‹œí€€ìŠ¤ ëª¨ë¸ê³¼ ë‹¬ë¦¬ ë‹¨ì–´ì˜ ë¬¸ì¥ ë‚´ ìœ„ì¹˜ ì •ë³´ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜ ì–´ìˆœì€ ë§¥ë½ì„ íŒŒì•…í•˜ëŠ”ë°ì— ì¤‘ìš”í•œ ë‹¨ì„œê°€ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë¬¸ì¥ ë‚´ì—ì„œ ê°™ì€ ë‹¨ì–´ê°€ ë“±ì¥í•´ë„ ê° ë‹¨ì–´ëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ë¦¬í‚¬ ìˆ˜ ìˆê³ , ì–¸ì–´ë§ˆë‹¤ ë¬¸ë²• êµ¬ì¡°ì— ë”°ë¼ ì–´ìˆœì´ ë‹¤ë¥´ë©°, ê°™ì€ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•´ë„ ì–´ìˆœì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ë‚´í¬í•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ìœ„ì¹˜ì— ë”°ë¥¸ ì„ì˜ì˜ ê°’ì„ ì„¤ì •í•´ Q, Kì™€ Vì˜ ì„ë² ë”©ì— **ë”í•˜ëŠ”**ë°, ì´ ê²ƒì„ positional encodingì´ë¼ê³  í•œë‹¤. Traxì—ì„œëŠ” ì—¬ëŸ¬ê°€ì§€ positional encoding ë°©ë²•ì„ ì§€ì›í•˜ê³  ìˆëŠ”ë° ([link](https://trax-ml.readthedocs.io/en/latest/trax.layers.html?highlight=positional%20encoding#module-trax.layers.research.position_encodings)), ì› ë…¼ë¬¸ì—ì„œëŠ” ì°¨ì› `i`ì™€ ìœ„ì¹˜ `pos`ì— ëŒ€í•œ sine ê³¡ì„ ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤.

$$
\begin{aligned}
PE_{(pos,2i)} &= sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} &= cos(pos/10000^{2i/d_{model}})
\end{aligned}
$$

ì¦‰ positional encodingì˜ ê° ì°¨ì›ì€ ì‚¬ì¸ ê³¡ì„ ì— ëŒ€ì‘í•œë‹¤. PEëŠ” $2\pi$ì—ì„œ ë¶€í„° $10000 \cdot 2\pi$ê¹Œì§€ì˜ ê¸°í•˜í•™ì  í˜•íƒœë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì„œ ìƒìˆ˜ $k$ì— ëŒ€í•´ ìƒëŒ€ì ì¸ ìœ„ì¹˜ì¸ $PE(pos+k)$ë¥¼ $PE(pos)$ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. trax.layers.Attentionì—ì„œ ì •ì˜í•˜ê³  ìˆëŠ” `PositionalEncoding` ë„ ê°™ì€ ë°©ë²•ì„ ì ìš©í–ˆë‹¤.

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `PositionalEncoding` ë ˆì´ì–´ë‹¤.

```python
class PositionalEncoding(base.Layer):
  
  # ...

  def forward(self, inputs):
    """Returns the input activations, with added positional information."""
    weights = self.weights

    # ...

    emb = fastmath.dynamic_slice_in_dim(
        weights, self.state, inputs.shape[1], axis=1)
    self.state += inputs.shape[1]
    return inputs + emb

  def init_weights_and_state(self, input_signature):
    """Randomly initializes the positional encoding vectors.
    """
    d_feature = input_signature.shape[-1]
    if self._d_feature is not None:
      d_feature = self._d_feature
    pe = np.zeros((self._max_len, d_feature), dtype=np.float32)
    position = np.arange(0, self._max_len)[:, np.newaxis]
    div_term = np.exp(
        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)  # [self._max_len, d_feature]
    if self._use_bfloat16:
      pe = pe.astype(jnp.bfloat16)
    w = jnp.array(pe)  # Trainable parameters, initialized above.
    # ...
```

`init_weights_and_state` í•¨ìˆ˜ëŠ” `ShapeDtype` ê°ì²´ë¥¼ ì…ë ¥ë°›ì•„ì„œ ì„ë² ë”© í¬ê¸° `(max_len, d_feature)` í¬ê¸°ì˜ ë²¡í„° `pe`ë¥¼ ì´ˆê¸°í™” í•œë‹¤. `pe` ë²¡í„°ì˜ ì§ìˆ˜ í–‰ì—ëŠ” `positon * div_term`ì˜ sine ê°’ì„ í• ë‹¹í•˜ê³  í™€ìˆ˜ í–‰ì—ëŠ” cosine ê°’ì„ í• ë‹¹í•œë‹¤. ì´ ê°’ì„ `weights`ë¡œ ì „ë‹¬í•´ í•¨ìˆ˜`forward`ì—ì„œ `emb` ê°’ìœ¼ë¡œ ì…ë ¥ê°’ì— ë”í•´ ì „ë‹¬í•˜ê³  ìˆë‹¤. ì¦‰ ì‚¬ì¸ê³¼ ì½”ì‚¬ì¸ ê°’ìœ¼ë¡œ ìœ„ì¹˜ì •ë³´ë¥¼ ì¸ì½”ë”©í•´ ì…ë ¥ê°’ì— ë”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ positional encodingì„ ìˆ˜í–‰í•œë‹¤.

### ğŸ“£ Encoder Self-Attention 

ì¸ì½”ë”ëŠ” ì…€í”„-ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ í™œìš©í•œë‹¤. ì…€í”„-ì–´í…ì…˜ì€ ì£¼ì–´ì§„ ë°ì´í„°ì˜ ë¶€ë¶„ê°’ê³¼ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰ ë¬¸ì¥ ë°ì´í„°ì—ì„œ ì…€í”„-ì–´í…ì…˜ì€ ë¬¸ì¥ ë‚´ì˜ ë‹¨ì–´ ë¬¸ë§¥ì„ íŒŒì•…í•œë‹¤. ì•ì„œ $Q$, $K$, $V$ì˜ ê°’ì€ ì–´í…ì…˜ì— ë”°ë¼ ë‹¤ë¥´ë‹¤ê³  í–ˆëŠ”ë°, ì…€í”„-ì–´í…ì…˜ ë ˆì´ì–´ì—ì„œëŠ” ëª¨ë“  $Q$, $K$, $V$ê°€ ê°™ì€ ì‹œí€€ìŠ¤, ì¦‰ ì¸ì½”ë”ì˜ ì´ì „ ë ˆì´ì–´ì˜ ê²°ê³¼ê°’ì—ì„œ ì˜¨ë‹¤. ì£¼ì–´ì§„ ë¬¸ì¥ì´ ìˆì„ ë•Œ, ì„ì˜ì˜ ë‹¨ì–´ $w$ì— ëŒ€ì‘í•˜ëŠ” ì„ë² ë”©ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ì„ë² ë”©ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ì–»ì€ ì¿¼ë¦¬ $q$ì— ëŒ€í•´ ëª¨ë“  $k \in K$ì™€ dot productë¡œ ë¹„êµí•´ì„œ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ ë‹¤ìŒ softmax í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ë”í•´ì„œ 1ì´ ë˜ëŠ” ì–‘ìˆ˜ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ê·¸ í›„ ë‘ë²ˆì§¸ dot productë¡œ ë‹¨ì–´ $w$ì— ëŒ€ì‘í•˜ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ $v$ê°’ë“¤ì„ êµ¬í•´ ë‹¤ì‹œ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•¨ìœ¼ë¡œì„œ ëª¨ë“  ê°€ì¤‘ì¹˜ í•©ì„ êµ¬í•œë‹¤. ì´ê²ƒì´ ë‹¨ì–´ $w$ì— ëŒ€í•œ ì–´í…ì…˜ì´ë‹¤.

![](img/attention2.png)
<center>
ì…€í”„-ì–´í…ì…˜, Vaswani et al., 2017
</center>

</br>

ìœ„ ê·¸ë¦¼ì€ _"making"_ ë‹¨ì–´ì— ëŒ€í•œ ì–´í…ì…˜ì„ í‘œí˜„í•˜ê³  ìˆë‹¤. ìœ„ ì–´í…ì…˜ì˜ $Q$, $K$, $V$ëŠ” ëª¨ë‘ í•œ ë¬¸ì¥ _"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult. <EOS> <pad> ..."_ì—ì„œ ì–»ì–´ì§„ë‹¤. ë‹¤ë¥¸ ìƒ‰ê¹”ì€ ë‹¤ë¥¸ headë¥¼ ë‚˜íƒ€ë‚´ë©° ìƒ‰ì´ ì„ ëª…í• ìˆ˜ë¡ ê´€ê³„ë„ê°€ ë†’ë‹¤. _"making"_ê³¼ ì—°ê´€ëœ headëŠ” _"making ... more difficult"_ êµ¬ë¬¸ì„ ì™„ì„±í•œë‹¤.

ë¬¼ë¡  íš¨ìœ¨ì„ ìœ„í•´ ìš°ë¦¬ëŠ” í–‰ë ¬ ë‹¨ìœ„ë¡œ ì–´í…ì…˜ì„ ì—°ì‚°í•œë‹¤. ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ë‹¨ì–´ ì„ë² ë”©ì´ ì„ë² ë”© ì°¨ì› embì— ëŒ€í•´ $(n_{seq}, emb)$ ì°¨ì›ì´ë¼ê³  í•˜ì. ìš°ë¦¬ëŠ” $(emb, d_{model})$ì°¨ì›ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W^Q$, $W^K$, ê·¸ë¦¬ê³  $W^V$ì„ í•™ìŠµí•œë‹¤. ë‹¨ì–´ì— ëŒ€í•´ í–ˆë˜ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì„ë² ë”© í–‰ë ¬ì— ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì„œ ê°ê° $(n_{seq}, d_{model})$ ì°¨ì›ì˜ í–‰ë ¬ $Q$, $K$, ê·¸ë¦¬ê³  $V$ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆë‹¤. ì´í›„ì˜ ì–´í…ì…˜ ì—°ì‚°ì€ scaled-dot product ì–´í…ì…˜ì—ì„œ ì‚´í´ë³¸ ê²ƒê³¼ ê°™ë‹¤. Multi-head ì…€í”„-ì–´í…ì…˜ì„ ì‹¤í–‰í•œë‹¤ë©´, $i \in n_{heads}$ì— ëŒ€í•´ $W^Q_i$, $W^K_i$, $W^V_i$ë¥¼ í›ˆë ¨í•˜ê³  $Q_i$, $K_i$, $V_i$ë¡œ ì–´í…ì…˜ì„ ì—°ì‚°í•œ í›„, ì–´í…ì…˜ ê²°ê³¼ê°’ $Z_i$ë¥¼ ê²°í•©í•œ $Z$ì— í•™ìŠµí•œ $(n_{seq}, d_v)$ ì°¨ì›ì˜ ê°€ì¤‘ì¹˜ $W^O$ í–‰ë ¬ì„ ê³±í•´ ìµœì¢…ì ìœ¼ë¡œ multi-head ì–´í…ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `Attention` ëª¨ë¸ë¡œ, Multi-Head ì…€í”„-ì–´í…ì…˜ì„ ìˆ˜í–‰í•œë‹¤.

```python
def Attention(d_feature, n_heads=1, dropout=0.0, mode='train'):
  """Returns a layer that maps `(vectors, mask)` to `(new_vectors, mask)`.
  This layer type represents one pass of multi-head self-attention, from vector
  set to vector set, using masks to represent out-of-bound (e.g., padding)
  positions. ...
  """
  return cb.Serial(
      cb.Select([0, 0, 0]),
      AttentionQKV(d_feature, n_heads=n_heads, dropout=dropout, mode=mode),
  )
```

ë””ì½”ë”ì—ì„œ ì‚´í´ë³´ê² ì§€ë§Œ, `AttentionQKV`ëŠ” $Q$, $K$, $V$ë¥¼ ë‹¤ë¥¸ ì…ë ¥ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì…€í”„-ì–´í…ì…˜ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ `Attention`ì€ `Select`ë¡œ ì²«ë²ˆì§¸ ì…ë ¥ê°’ì„ 3ê°œë¡œ ë³µì œí•œ ê°’ì„ `AttentionQKV`ì— ì „ë‹¬í•œë‹¤.

ë˜ ì¸ì½”ë”ì˜ ì…€í”„-ì–´í…ì…˜ì€ **Padding Mask**ë¥¼ í™œìš©í•œë‹¤. 

```python
def PaddingMask(pad=0):
  """Returns a layer that maps integer sequences to padding masks.
  The layer expects as input a batch of integer sequences. The layer output is
  an N-D array that marks for each sequence position whether the integer (e.g.,
  a token ID) in that position represents padding -- value ``pad`` -- versus
  text/content -- all other values. The padding mask shape is
  (batch_size, 1, 1, encoder_sequence_length), such that axis 1 will broadcast
  to cover any number of attention heads and axis 2 will broadcast to cover
  decoder sequence positions. ...
  """
  def f(x):
    if len(x.shape) != 2:
      raise ValueError(
          f'Input to PaddingMask must be a 2-D array with shape '
          f'(batch_size, sequence_length); instead got shape {x.shape}.')
    batch_size = x.shape[0]
    sequence_length = x.shape[1]
    content_positions = (x != pad)
    return content_positions.reshape((batch_size, 1, 1, sequence_length))
  return Fn(f'PaddingMask({pad})', f)
```

ì¦‰ íŒ¨ë”© í† í° `pad`ë¡œ ì„¤ì •ëœ ê°’ê³¼ ê°™ì€ ë¶€ë¶„ì„ $0$ìœ¼ë¡œ ë°”ê¾¼ë‹¤. ì¶œë ¥ ì°¨ì›ì€ `(batch_size, 1, 1, sequence_length)`ìœ¼ë¡œ, ì–´í…ì…˜ê³¼ ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ 1ë²ˆê³¼ 2ë²ˆ ì¶•ì„ ì¶”ê°€í•œë‹¤.


## 2. Decoder

íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë”ëŠ” ë‘ ê°€ì§€ ì–´í…ì…˜ì„ ê±°ì¹œë‹¤. ì²« ë²ˆì§¸ ì–´í…ì…˜ì€ ì¸ì½”ë”ì—ì„œì™€ ê°™ì€ ì…€í”„-ì–´í…ì…˜ì´ê³ , ë‘ë²ˆì§¸ëŠ” ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ì´ë‹¤. ì¸ì½”ë”ë§Œ ì‚¬ìš©í•´ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆì—ˆë˜ ê²ƒì²˜ëŸ¼, ë””ì½”ë”ë§Œ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ í˜•ì„±í•  ìˆ˜ë„ ìˆë‹¤. ë””ì½”ë”ë§Œ ì‚¬ìš©í•  ë•Œì—ëŠ” Multi-Head ì…€í”„-ì–´í…ì…˜ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

![](img/transformer_decoder.png)
<center>
ë””ì½”ë” êµ¬ì¡°, from deeplearning.ai
</center>

</br>

ì¸ì½”ë”ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë””ì½”ë”ì—ì„œë„ ì…ë ¥ê°’ $Q$, $K$, $V$ë¥¼ ì„ë² ë“œí•˜ê³  Positional Encoding ì²˜ë¦¬ë¥¼ í•œ í›„ì— Residualì„ í¬í•¨í•œ 1ï¸âƒ£ Multi-Head Attentionê³¼ 2ï¸âƒ£ FeedForward ë ˆì´ì–´ë¥¼ ê±°ì¹œë‹¤. ë””ì½”ë” ë¸”ëŸ­ì„ ì—¬ëŸ¬ë²ˆ ê±°ì¹œ í›„ì— í›ˆë ¨ ê°€ëŠ¥í•œ Linear ë ˆì´ì–´ì™€ Softmax í•¨ìˆ˜ë¥¼ ê±°ì¹˜ëŠ”ë°, ì´ ë¶€ë¶„ì€ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ê³¼ì œì— ë”°ë¼ ë³€ê²½í•  ìˆ˜ ìˆë‹¤.

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `TransformerLM`ë¡œ, ë””ì½”ë”ë§Œ êµ¬í˜„ëœ í•¨ìˆ˜ì´ë‹¤.

```python
def TransformerLM(vocab_size,
                  d_model=D_MODEL,
                  d_ff=D_FF,
                  n_layers=N_LAYERS,
                  n_heads=N_HEADS,
                  max_len=MAX_SEQUENCE_LENGTH,
                  dropout=DROPOUT_RATE,
                  dropout_shared_axes=DROPOUT_SHARED_AXES,
                  mode=MODE,
                  ff_activation=FF_ACTIVATION_TYPE):

  # ...

  def _DecBlock():
    return _DecoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,
                         mode, ff_activation)

  return tl.Serial(
      tl.ShiftRight(mode=mode),  # Teacher Forcing
      tl.Embedding(vocab_size, d_model),
      _Dropout(),
      tl.PositionalEncoding(max_len=max_len, mode=mode),
      [_DecBlock() for _ in range(n_layers)],
      tl.LayerNorm(),
      tl.Dense(vocab_size),
  )
```

`_DecoderBlock`ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
def _DecoderBlock(d_model,
                  d_ff,
                  n_heads,
                  dropout,
                  dropout_shared_axes,
                  mode,
                  ff_activation):
  # ...

  return [
      tl.Residual(
          tl.LayerNorm(),
          _CausalAttention(),
          _Dropout(),
      ),
      tl.Residual(
          tl.LayerNorm(),
          _FFBlock(),
          _Dropout(),
      ),
  ]
```

`TransformerLM` í•¨ìˆ˜ëŠ” Teacher Forcingì„ ê±°ì³ ì„ë² ë”©, positional encoding ì²˜ë¦¬ í›„ `n_layers`ë§Œí¼ì˜ ë””ì½”ë” ë¸”ëŸ­ì„ ê±°ì³ `tl.LayerNorm()`ê³¼ `vocab_size`ë§Œí¼ì˜ `tl.Dense()` ë ˆì´ì–´ì„ í†µê³¼í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì‚¬ì „ì— ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ì„ í†µí•´ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ vocabulary ë‚´ì˜ í† í°ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì–¸ì–´ ëª¨ë¸(language model)ì„ ìˆ˜í–‰í•œë‹¤.


### ğŸ”† Teacher Forcing
ë””ì½”ë” ë¸”ëŸ­ì— ë“¤ì–´ê°€ê¸°ì— ì•ì„œ, Teacher Forcing ê¸°ë²•ì„ í™œìš©í•´ ëª¨ë¸ì˜ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤. RNN ëª¨ë¸ì¸ Seq2Seq ëª¨ë¸ì€ ë°”ë¡œ ì „ ë ˆì´ì–´ì˜ ì˜ˆì¸¡ ê°’ì„ ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ë•Œë¬¸ì— ëª¨ë¸ í›ˆë ¨ ì´ˆê¸°ì˜ (ëœ í›ˆë ¨ëœ) ë‚˜ìœ ì˜ˆì¸¡ ê°’ì´ ê³„ì†í•´ì„œ ëª¨ë¸ í›ˆë ¨ì— ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì´ì „ ë ˆì´ì–´ì˜ ì˜ˆì¸¡ê°’ì´ ì•„ë‹Œ ì‹¤ì œ íƒ€ì¼“ ê°’ì„ ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ Teacher Forcingì´ë¼ê³  í•œë‹¤. ë§ˆì¹˜ ì„ ìƒë‹˜ì´ ì§ì ‘ ì´ë ‡ê²Œ í•˜ë¼ê³  ì§€ë„í•´ì£¼ëŠ” ê²ƒê³¼ ê°™ë‹¤. í›ˆë ¨ ì´ˆê¸°ì— íƒ€ê²Ÿ ê°’ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë•ê¸° ë•Œë¬¸ì—, ì´ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨ ì†ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ ë†’ì¼ ìˆ˜ ìˆë‹¤.

ìœ„ì˜ ì½”ë“œì—ì„œ ë‚˜íƒ€ë‚œ `ShiftRight` ë ˆì´ì–´ê°€ teacher forcing ì—­í• ì„ í•œë‹¤. ì¦‰ (í•œ ì‹œì  ë¯¸ë˜ ê°’ì¸) ë°”ë¡œ ì˜¤ë¥¸ ìª½ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ êµì •í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œ `mode`ê°€ ì¸ìë¡œ ë“¤ì–´ê°„ ì´ìœ ë„, í•™ìŠµ ì™¸ì— ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ë•ŒëŠ” teacher forcingì„ ì ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.

ê·¸ë ‡ì§€ë§Œ Teacher Forcingì€ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê³¼ì •ì—ì„œ ì‹¤ì œ íƒ€ê²Ÿê°’ì„ ë…¸ì¶œí•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ ì•ˆì •ì„±, ì¦‰ ë³´ë‹¤ ì¼ë°˜ì ì¸ ì˜ˆì— ëŒ€í•œ ì˜ˆì¸¡ ëŠ¥ë ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ í›ˆë ¨ ì¤‘ì— Labelì— ë…¸ì¶œë˜ëŠ” ê²½ìš°ë¥¼ **Exposure Bias**ê°€ ìˆë‹¤ê³  í•œë‹¤. ì´ ë•Œë¬¸ì— curriculum learning ë°©ë²•ì—ì„œëŠ” FeedForwardì˜ í•™ìŠµ ì´ˆê¸°ì—ë§Œ ì´ì „ ë ˆì´ì–´ì˜ ì˜ˆì¸¡ê°’ì„ íƒ€ê²Ÿ ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  í•™ìŠµ í›„ê¸°ì—ëŠ” ëŒ€ì²´ í•˜ì§€ ì•ŠëŠ”ë‹¤.

### ğŸ“£ Causal Self-Attention

ë””ì½”ë”ì˜ ì…ë ¥ê°’ì— ëŒ€í•œ ì–´í…ì…˜ì„ ì‹¤í–‰í•  ë•Œë„ ì–´í…ì…˜ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë¬¸ë§¥ì„ ìœ„í•´ ì…€í”„-ì–´í…ì…˜ì„ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤. ë‹¨, ë””ì½”ë”ì˜ ì…€í”„-ì–´í…ì…˜ì€ **Causal Mask**ê°€ í•„ìš”í•˜ë‹¤. ì•ì—ì„œì™€ ê°™ì´ ê¸°ê³„ ë²ˆì—­ ê³¼ì œë¥¼ ê³ ë ¤í•´ë³´ì. RNNì€ ë””ì½”ë”ì˜ ì…ë ¥ê°’ì— ìˆœì°¨ì ìœ¼ë¡œ ì ‘ê·¼í•´ ë§¤ë²ˆ ì¸ì½”ë”ì˜ ê²°ê³¼ê°’ê³¼ í•´ë‹¹ ì‹œì ì˜ ë””ì½”ë” ì…ë ¥ê°’ì„ ë¹„êµí•  ê²ƒì´ë‹¤. ê·¸ë ‡ì§€ë§Œ ì–´í…ì…˜ì€ ëª¨ë“  ì‹œì ì˜ ë°ì´í„°ë¥¼ í•œë²ˆì— ë³¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŠ¹ì • ì‹œì ì—ì„œ ëª¨ë¸ì˜ íƒ€ê¹ƒì¸ ì˜¤ë¥¸ìª½ ê°’ì— ëŒ€í•œ ì ‘ê·¼(attend)ì„ ë°©ì§€í•´ì•¼ í•œë‹¤.

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Traxì˜ `_causal_mask`ë¡œ, ì¸ìë¡œ ë°›ì€ `length` ê¸¸ì´ ì •ë°© í–‰ë ¬ì˜ lower triangular í–‰ë ¬ì„ ë°˜í™˜í•œë‹¤.  

```python
def _causal_mask(length):
  # Not all backends define jnp.tril. However, using np.tril is inefficient
  # in that it creates a large global constant. TODO(kitaev): try to find an
  # alternative that works across all backends.
  if fastmath.is_backend(fastmath.Backend.JAX):
    return jnp.tril(jnp.ones((1, length, length), dtype=np.bool_), k=0)
  else:
    return np.tril(np.ones((1, length, length), dtype=np.bool_), k=0)
```

`DotProductCausalAttention` ì–´í…ì…˜ì€ 1ê°œ Head ì–´í…ì…˜ì„ êµ¬í˜„í•˜ëŠ” í•¨ìˆ˜ë¡œ `CausalAttention`ìœ¼ë¡œ êµ¬í˜„ë  ìˆ˜ ìˆë‹¤. ì£¼ëª©í•´ì„œ ë³¼ ì ì€ ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì„ ë•Œë§Œ Causal Maskë¥¼ í™œìš©í•˜ëŠ” ì ì´ë‹¤.

```python
class DotProductCausalAttention(base.Layer):
  """Layer that computes attention strengths by masking out the "future".
  Causal attention uses masking to prevent a given sequence position from
  attending to positions greater than / following it. This is used, for
  example, when training autoregressive sequence models, or when decoding a
  sequence symbol by symbol.
  This layer performs the core per-head attention calculation. The layer
  assumes that any splitting into attention heads precedes it, and that any
  merging of attention heads will follow it.
  """
  # ...

  def forward(self, inputs):
    """Returns attention-computed activations.
    Args:
      inputs: A (queries, keys, values) tuple.
    """
    q, k, v = inputs

    # ...

    if self._mode == 'predict':
      self.state, mask = _fast_inference_update_state(
          inputs, self.state,
          mask_for_predict=mask_for_predict)
      # ...
    else:
      sequence_length = q.shape[-2]
      mask = _causal_mask(sequence_length)

    activations, attn_strengths = _per_head_attention(
        q, k, v, mask, dropout=self._dropout, mode=self._mode, rng=self.rng)
    #...
    return activations

    # ...
```


### ğŸ“£ Encoder-Decoder Attention

ì´ì œ ë””ì½”ë”ì˜ ì…ë ¥ê°’ê³¼ ì¸ì½”ë”ì˜ ì…ë ¥ê°’ì„ ë¶„ì„í•˜ëŠ” ê³¼ì œê°€ ë‚¨ì•˜ë‹¤. ì¸ì½”ë”-ë””ì½”ë” ë¸”ëŸ­ì˜ ì…ë ¥ê°’ì€ `(vec_d, mask, vec_e)`ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ `mask`ë¥¼ ì´ìš©í•´ íŒ¨ë”©ëœ ê°’ì— ëŒ€í•´ì„œëŠ” ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. 

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `AttentionQKV` í•¨ìˆ˜ë¡œ, `Attention`ì´ ì…€í”„-ì–´í…ì…˜ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬ $Q$ì™€ $K-V$ë¥¼ ë‹¤ë¥¸ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ í—ˆìš©í•œë‹¤. 

```python
def AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train',
                 cache_KV_in_predict=False, q_sparsity=None,
                 result_sparsity=None):
  # ...

  return cb.Serial(
      cb.Parallel(_SparsifiableDense(q_sparsity),
                  _CacheableDense(),
                  _CacheableDense()),
      _PureAttention(),
      _SparsifiableDense(result_sparsity),
  )
```

`cb.Parallel`ì€ $Q$ì— í•´ë‹¹í•˜ëŠ” ì…ë ¥ì„ ë°€ë„ `q_sparsity`ë¥¼ ê°€ì§€ëŠ” í–‰ë ¬ë¡œ, ê·¸ë¦¬ê³  $K$ì™€ $V$ì— í•´ë‹¹í•˜ëŠ” ì…ë ¥ì„ `d_feature`ë§Œí¼ì˜ `Dense` ë ˆì´ì–´ë¡œ ë‘ê³ , `_PureAttention`ê³¼ ì¼ì¢…ì˜ í›ˆë ¨ê°€ëŠ¥í•œ `Dense` ë ˆì´ì–´ë¥¼ í†µê³¼í•œë‹¤.

ìµœì¢…ì ìœ¼ë¡œ Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `_EncoderDecoderBlock` í•¨ìˆ˜ë¥¼ ë³´ì.

```python
def _EncoderDecoderBlock(d_model,
                         d_ff,
                         n_heads,
                         dropout,
                         dropout_shared_axes,
                         mode,
                         ff_activation):

  def _Dropout():
    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)

  def _AttentionQKV():
    return tl.AttentionQKV(d_model, n_heads=n_heads, dropout=dropout,
                           mode=mode, cache_KV_in_predict=True)

  def _CausalAttention():
    return tl.CausalAttention(d_model, n_heads=n_heads, mode=mode)

  def _FFBlock():
    return _FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode,
                             ff_activation)

  return [                             # vec_d masks vec_e
      tl.Residual(
          tl.LayerNorm(),
          _CausalAttention(),
          _Dropout(),
      ),
      tl.Residual(
          tl.LayerNorm(),
          tl.Select([0, 2, 2, 1, 2]),  # vec_d vec_e vec_e masks vec_e
          _AttentionQKV(),             # vec_d masks vec_e
          _Dropout(),
      ),
      tl.Residual(
          tl.LayerNorm(),
          _FFBlock(),
          _Dropout(),
      ),
  ]
```

`_EncoderDecoderBlock`ì€ ë””ì½”ë” ë²¡í„°, ë§ˆìŠ¤í¬, ì¸ì½”ë” ë²¡í„° ìŒì¸ `(vec_d, masks, vec_e)`ë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤. ì—¬ê¸°ì„œ ë””ì½”ë”ì˜ ë‘ê°€ì§€ ì–´í…ì…˜ì„ ëª¨ë‘ ì‹¤í–‰í•˜ê³  ìˆìŒì— ìœ ì˜í•œë‹¤. ì²«ì§¸ë¡œ `_CausalAttention`ì„ ì ìš©í•œ í›„ `(vec_d, vec_e, vec_e, masks, vec_e)`ë¡œ ë°ì´í„°ë¥¼ ì •ë ¬í•œë‹¤. ì•ì—ì„œ ë¶€í„° ì„¸ ê°œ ì…ë ¥ì´ `_AttentionQKV()`ë ˆì´ì–´ë¥¼ ê±°ì³ì„œ ì…ë ¥ê°’ê³¼ ê°™ì€ ì°¨ì›ì¸ `(vec_d, masks, vec_e)`ë¥¼ ì–»ì–´ FeedForward ë¸”ëŸ­ì„ ê±°ì¹œë‹¤. ì¦‰ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ì—ì„œëŠ” ë””ì½”ë” ë²¡í„°ë¥¼ $Q$ë¡œ, ì¸ì½”ë” ë²¡í„°ë¥¼ $K$ì™€ $V$ë¡œ ì…ë ¥ë°›ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.


## 4. Overall

ë‹¤ìŒì€ ì¸ì½”ë”©ê³¼ ë””ì½”ë”©ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ”ê·¸ë¦¼ìœ¼ë¡œ, ì˜ì–´ ë¬¸ì¥ì„ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ìœ¼ë¡œ ë²ˆì—­í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì´ê³ ìˆë‹¤.

![](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif) 
*Cool gif from [Google AI blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)*

Transformer ëª¨ë¸ì€ ì¸ì½”ë”ë‚˜ ë””ì½”ë”ë§Œìœ¼ë¡œë„ ì“°ì„ì´ ìˆì§€ë§Œ, ê¸°ê³„ ë²ˆì—­ê³¼ ê°™ì€ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ê³¼ì œì—ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ëª¨ë‘ í™œìš©í•´ì•¼ í•œë‹¤. ì•„ë˜ì˜ `Transformer` ëª¨ë¸ì€ ì•ì„œ ì‚´í´ ë³¸ ì¸ì½”ë”ì™€ ë””ì½”ë” ë¸”ëŸ­ì„ í™œìš©í•´ ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ë°˜í™˜í•œë‹¤.

ğŸ“‚ ë‹¤ìŒ ì½”ë“œëŠ” Trax ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `Transformer` ëª¨ë¸ì´ë‹¤.

```python
def Transformer(input_vocab_size,
                output_vocab_size=None,
                d_model=D_MODEL,
                d_ff=D_FF,
                n_encoder_layers=N_LAYERS,
                n_decoder_layers=N_LAYERS,
                n_heads=N_HEADS,
                max_len=MAX_SEQUENCE_LENGTH,
                dropout=DROPOUT_RATE,
                dropout_shared_axes=DROPOUT_SHARED_AXES,
                mode=MODE,
                ff_activation=FF_ACTIVATION_TYPE):
  """Returns a full Transformer model.
  This model is an encoder-decoder that performs tokenized string-to-string
  ("source"-to-"target") transduction:
  """

  # ...

  def _Dropout():
    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)

  def _EncBlock():
    return _EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,
                         mode, ff_activation)

  def _Encoder():
    encoder = tl.Serial(
        in_embedder,
        _Dropout(),
        tl.PositionalEncoding(max_len=max_len, mode=encoder_mode),
        [_EncBlock() for _ in range(n_encoder_layers)],
        tl.LayerNorm(),
    )
    return tl.Cache(encoder) if mode == 'predict' else encoder

  def _EncDecBlock():
    return _EncoderDecoderBlock(d_model, d_ff, n_heads, dropout,
                                dropout_shared_axes, mode, ff_activation)

  # Input to model is encoder-side tokens and decoder-side tokens: tok_d, tok_e
  # Model output is decoder-side vectors and decoder-side tokens: vec_d  tok_d
  return tl.Serial(
      tl.Select([0, 1, 1]),  # Copies decoder tokens for use in loss.

      # Encode.
      tl.Branch([], tl.PaddingMask()),  # tok_e masks tok_d tok_d
      _Encoder(),

      # Decode.
      tl.Select([2, 1, 0]),  # Re-orders inputs: tok_d masks vec_e .....
      tl.ShiftRight(mode=mode),
      out_embedder,
      _Dropout(),
      tl.PositionalEncoding(max_len=max_len, mode=mode),
      tl.Branch([], tl.EncoderDecoderMask()),  # vec_d masks ..... .....
      [_EncDecBlock() for _ in range(n_decoder_layers)],
      tl.LayerNorm(),
      tl.Select([0], n_in=3),  # Drops masks and encoding vectors.

      # Map vectors to match output vocab size.
      tl.Dense(output_vocab_size),
  )
```

`Transformer` ëŠ” ì¸ì½”ë”ì— ì…ë ¥ë˜ëŠ” í† í°ê³¼ ë””ì½”ë”ì— ì…ë ¥ë˜ëŠ” í† í° ìŒ `(tok_d, tok_e)`ë¥¼ ì…ë ¥ë°›ì•„ ë””ì½”ë” ë²¡í„°ì™€ ë””ì½”ë” í† í° ìŒ `(vec_d, tok_d)`ë¥¼ ë°˜í™˜í•œë‹¤. `tl.Branch`ë¡œ ì…ë ¥ê°’ê³¼ íŒ¨ë”©ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•œ í›„ `_Encoder()`ë¡œ ì¸ì½”ë”©ì„ ì‹¤í–‰í•œë‹¤. `_Encoder`ëŠ” `n_encoder_layers`ê°œì˜ ì¸ì½”ë” ë¸”ëŸ­ `_EncoderDecoderBlock`ì„ í¬í•¨í•˜ë„ë¡ ì •ì˜ë˜ì–´ ìˆë‹¤. ì¸ì½”ë”©ì„ ê±°ì¹˜ë©´ ë°ì´í„°ëŠ” `(vec_e, masks, tok_d)`ê°€ ë˜ë©° `tl.Select`ë¡œ ìˆœì„œë¥¼ ë’¤ì§‘ì–´ teacher forcingê³¼ positional encodingì„ ì‹¤í–‰í•œë‹¤. ë‘ë²ˆì§¸ `tl.Branch`ë¡œ `(tok_d, masks)`ë¥¼ ì¸ì½”ë”-ë””ì½”ë” ë¸”ëŸ­ì— ì…ë ¥í•˜ë©°, ì´ ì™¸ì˜ ê°’ë“¤ì€ ì´í›„ `tl.Select`ë¡œ ì œì™¸ì‹œí‚¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. `_EncoderDecoderBlock`ì€ ì•ì—ì„œ ì‚´í´ë³¸ ëŒ€ë¡œë‹¤. ì´ë¡œì¨ `Transformer`í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒë§Œìœ¼ë¡œ  íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.


## ì°¸ê³  ìë£Œ
1. Natural Language Processing with Attention Models, deeplearning.ai, Coursera, https://www.coursera.org/specializations/natural-language-processing
2. Trax Library for Machine Learning, Github, https://github.com/google/trax
3. Transformers, Google AI blog, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
4. Vaswani et al, 2017, https://arxiv.org/abs/1706.03762
5. Jay Alammar on Github page, https://jalammar.github.io/illustrated-transformer/
